{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "s4_train_face_trainer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKDzxwVueEkU"
      },
      "source": [
        "## Evaluate the trained network (Week 12) - Step 4\n",
        "\n",
        "####**Designed by Joon Son Chung, November 2020**\n",
        "\n",
        "This is based on https://github.com/joonson/face_trainer. You should read the code if you want to understand more about the training details.\n",
        "\n",
        "In this step, we train the network on the generated dataset.\n",
        "\n",
        "The baseline model, available from [here](http://www.robots.ox.ac.uk/~joon/data/res18_vggface1_baseline.model), is trained on the VGGFace1 dataset using the softmax loss.\n",
        "\n",
        "The training and validation sets should also be in the experiments folder.\n",
        "\n",
        "`save_path` should be changed every time you run a new experiment.\n",
        "\n",
        "\n",
        "**The models take up a significant amount of disk space. Make sure that you have enough space on your Google Drive, and delete any unnecessary/ unsuccessful experiments.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRz0Qx2JeCls"
      },
      "source": [
        "from google.colab import drive\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm\n",
        "import os, glob, sys, shutil, time\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "\n",
        "# mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# path of the data directory relative to the home folder of Google Drive\n",
        "GDRIVE_HOME = '/content/drive/My Drive'\n",
        "FOLDER      = 'MLVU/your_dataset'\n",
        "\n",
        "# The following 4 lines are the only parts of the code that you need to change. You can simply run the rest.\n",
        "data_dir      = os.path.join(GDRIVE_HOME,FOLDER) ## path of the general experiment\n",
        "initial_model = os.path.join(GDRIVE_HOME,'MLVU/res18_vggface1_baseline.model') ## path to the pre-trained model\n",
        "train_zip     = os.path.join(data_dir,'train_set.zip') ## training data as zip\n",
        "val_zip       = os.path.join(data_dir,'val_set.zip') ## validation data as zip\n",
        "save_path     = os.path.join(data_dir,'experiment_v1') ## training logs and trained models will be saved here\n",
        "\n",
        "# Extract the cropped images\n",
        "with ZipFile(train_zip, 'r') as zipObj:\n",
        "  zipObj.extractall(\"/train_set\")\n",
        "with ZipFile(val_zip, 'r') as zipObj:\n",
        "  zipObj.extractall(\"/val_set\")\n",
        "print('Zip extraction complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H34dIrXThcXj"
      },
      "source": [
        "Make sure that the files have been extracted properly. Make sure that this is a reasonable number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlZoUu8YcY2X"
      },
      "source": [
        "train_files = glob.glob('/train_set/*/*.jpg')\n",
        "val_files   = glob.glob('/val_set/*/*.jpg')\n",
        "print(len(train_files),'train set files and',len(val_files),'validation set files found.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbh2C1LW5vHJ"
      },
      "source": [
        "First, clone the face recognition trainer from GitHub and add it to path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSpkzVfubKbV"
      },
      "source": [
        "!rm -rf face_trainer\n",
        "!git clone https://github.com/joonson/face_trainer.git\n",
        "\n",
        "sys.path.append('face_trainer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3djEvathdr4t"
      },
      "source": [
        "The training script. Please do not change, but try to read and understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQIfpumRdsM1"
      },
      "source": [
        "import datetime\n",
        "from utils import *\n",
        "from EmbedNet import *\n",
        "from DatasetLoader import get_data_loader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# ## ===== ===== ===== ===== ===== ===== ===== =====\n",
        "# ## Trainer script\n",
        "# ## ===== ===== ===== ===== ===== ===== ===== =====\n",
        "\n",
        "def train_network(args):\n",
        "\n",
        "    ## Make folders to save the results and models\n",
        "    args.model_save_path     = args.save_path+\"/model\"\n",
        "    args.result_save_path    = args.save_path+\"/result\"\n",
        "\n",
        "    if not(os.path.exists(args.model_save_path)):\n",
        "        os.makedirs(args.model_save_path)\n",
        "            \n",
        "    if not(os.path.exists(args.result_save_path)):\n",
        "        os.makedirs(args.result_save_path)\n",
        "\n",
        "    ## Load models\n",
        "    s = EmbedNet(**vars(args)).cuda();\n",
        "\n",
        "    ## Write args to scorefile\n",
        "    scorefile = open(args.result_save_path+\"/scores.txt\", \"a+\");\n",
        "\n",
        "    strtime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    scorefile.write('%s\\n%s\\n'%(strtime,args))\n",
        "    scorefile.flush()\n",
        "\n",
        "    ## Input transformations for training\n",
        "    train_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Resize(256),\n",
        "         transforms.RandomCrop([224,224]),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    ## Input transformations for evaluation\n",
        "    test_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Resize(256),\n",
        "         transforms.CenterCrop([224,224]),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    ## Initialise trainer and data loader\n",
        "    trainLoader = get_data_loader(transform=train_transform, **vars(args));\n",
        "    trainer     = ModelTrainer(s, **vars(args))\n",
        "    \n",
        "    ## If initial model is specified, start from that model\n",
        "    if(args.initial_model != \"\"):\n",
        "        trainer.loadParameters(args.initial_model);\n",
        "        print(\"Model %s loaded!\"%args.initial_model);\n",
        "\n",
        "    besteer   = 100\n",
        "    bestmodel = ''\n",
        "\n",
        "    ## Core training script\n",
        "    for it in range(1,args.max_epoch+1):\n",
        "\n",
        "        clr = [x['lr'] for x in trainer.__optimizer__.param_groups]\n",
        "\n",
        "        print(\"Training epoch %d with LR %f \"%(it,max(clr)));\n",
        "\n",
        "        loss, traineer = trainer.train_network(trainLoader, verbose=True);\n",
        "\n",
        "        if it % args.test_interval == 0:\n",
        "\n",
        "            snapshot = args.model_save_path+\"/model%09d.model\"%it\n",
        "            \n",
        "            sc, lab = trainer.evaluateFromList(transform=test_transform, **vars(args))\n",
        "            result = tuneThresholdfromScore(sc, lab, [1, 0.1]);\n",
        "\n",
        "            print(\"IT %d, VEER %2.4f\"%(it, result[1]));\n",
        "            scorefile.write(\"IT %d, VEER %2.4f\\n\"%(it, result[1]));\n",
        "\n",
        "            trainer.saveParameters(snapshot);\n",
        "\n",
        "            if result[1] < besteer:\n",
        "                besteer   = result[1]\n",
        "                bestmodel = snapshot\n",
        "\n",
        "        print(\"TEER/TAcc %2.2f, TLOSS %f\"%( traineer, loss));\n",
        "        scorefile.write(\"IT %d, TEER/TAcc %2.2f, TLOSS %f\\n\"%(it, traineer, loss));\n",
        "\n",
        "        scorefile.flush()\n",
        "\n",
        "    scorefile.close();\n",
        "\n",
        "    print('Best validation EER: %2.4f, %s'%(besteer,bestmodel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPR-wqIRPEuZ"
      },
      "source": [
        "Specify the input arguments to the trainer below, and run to train the network. The validation losses will be printed below, and also saved to `save_path`.\n",
        "\n",
        "See [here](https://github.com/joonson/face_trainer/blob/6fd64d96a1195c18689b1755853eea2082091819/trainEmbedNet.py#L18-L65) for the meanings of each of these arguments, and see [here](https://github.com/joonson/face_trainer/blob/main/README.md#implemented-loss-functions) for the list of available loss functions. If you use meta-learning loss functions, `nPerClass` must be 2 or more.\n",
        "\n",
        "Note that the trainer includes a script to make sure that there are only `nPerClass` images per class per mini-batch. This helps with the meta-learning loss functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blMxjMJYM5QG"
      },
      "source": [
        "import easydict \n",
        "args = easydict.EasyDict({ \"batch_size\": 20, # batch size\n",
        "                          \"trainfunc\": \"softmax\", # loss function\n",
        "                          \"lr\": 0.001, # learning rate\n",
        "                          \"lr_decay\": 0.90, # how much to decrease the learning rate, every 5 epochs\n",
        "                          \"weight_decay\": 0, # regularization to reduce overfitting (e.g. 1e-4 might be reasonable)\n",
        "                          \"margin\": 0.1, # for AM-softmax and AAM-softmax\n",
        "                          \"scale\": 30, # for AM-softmax and AAM-softmax\n",
        "                          \"nPerClass\": 1, # support set + query set size for meta-learning\n",
        "                          \"nClasses\": 1000, # number of identities in the training dataset\n",
        "                          # Don't change below here!!\n",
        "                          \"save_path\": save_path,\n",
        "                          \"max_img_per_cls\": 500, \n",
        "                          \"nDataLoaderThread\": 5, \n",
        "                          \"test_interval\": 3, \n",
        "                          \"max_epoch\": 30, \n",
        "                          \"optimizer\": \"adam\",\n",
        "                          \"scheduler\": \"steplr\",\n",
        "                          \"hard_prob\": 0.5,\n",
        "                          \"hard_rank\": 10,\n",
        "                          \"initial_model\": initial_model,\n",
        "                          \"train_path\": \"/train_set\",\n",
        "                          \"train_ext\": \"jpg\",\n",
        "                          \"test_path\": \"/val_set\",\n",
        "                          \"test_list\": \"/val_set/test_list.csv\",\n",
        "                          \"model\": \"ResNet18\",\n",
        "                          \"nOut\": 512,\n",
        "                          \"mixedprec\": False})\n",
        "        \n",
        "train_network(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm4Xc7Ls38L6"
      },
      "source": [
        "Try various experiments and record the results.\n",
        "\n",
        "\n",
        "| # | Params                   | Val EER | Notes |\n",
        "|---|--------------------------|---------|-------|\n",
        "| 1 | softmax / batch_size 200 |         |       |\n",
        "| 2 |                          |         |       |\n",
        "| 3 |                          |         |       |\n",
        "| 4 |                          |         |       |\n",
        "| 5 |                          |         |       |"
      ]
    }
  ]
}